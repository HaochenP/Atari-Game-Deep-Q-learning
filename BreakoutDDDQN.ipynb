{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv719uBL9x8t"
      },
      "outputs": [],
      "source": [
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bP4uW4_6BYy",
        "outputId": "e943c61d-447a-4c33-8a0b-cab8c2250712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.8/dist-packages (0.26.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.8.0\n",
            "  Downloading ale_py-0.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 19.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (5.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.5.0.tar.gz (10 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.11.0)\n",
            "Collecting libtorrent\n",
            "  Using cached libtorrent-2.0.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.9.24)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.5.0-py3-none-any.whl size=440868 sha256=0039665a37f6d2ca16453bf0192a329b6dd7b708a565be1677382a51ace1da4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/c9/25/578470ae932b494c313dc22e6c57afff192140fb3cd5acf185\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: libtorrent, AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.5.0 ale-py-0.8.0 autorom-0.4.2 libtorrent-2.0.7\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D98CddQuwKG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "#import wandb\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "import cv2\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "PtS7Z6p996ek",
        "outputId": "68ac0e7b-cbc7-4b1d-d290-2499b523e4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tornado/httputil.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  class HTTPHeaders(collections.MutableMapping):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220506_145055-yw0y4vx1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/adundee/Breakout-Pytorch/runs/yw0y4vx1\" target=\"_blank\">bright-blaze-16</a></strong> to <a href=\"https://wandb.ai/adundee/Breakout-Pytorch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/adundee/Breakout-Pytorch/runs/yw0y4vx1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fe9b7eb5c10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\"\"\"\n",
        "wandb.init(\n",
        "  project=\"Breakout-Pytorch\",\n",
        "  tags=[\"DQN\", \"CNN\", \"RL\"],\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8rGMlN2uzgk"
      },
      "outputs": [],
      "source": [
        "GPU = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxF5-bzUu1q-"
      },
      "outputs": [],
      "source": [
        "class DDQN(nn.Module):\n",
        "    def __init__(self, h, w, output_size):\n",
        "        super(DDQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
        "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
        "\n",
        "        # Action layer\n",
        "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
        "\n",
        "        # State Value layer\n",
        "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            #m.bias.data.fill_(0.0)\n",
        "        \n",
        "        if type(m) == nn.Conv2d:\n",
        "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            m.bias.data.fill_(0.1)\n",
        "\n",
        "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
        "        \"\"\"\n",
        "        Calcs conv layers output image sizes\n",
        "        \"\"\"\n",
        "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
        "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
        "        return next_w, next_h\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
        "\n",
        "        Ax = self.Alrelu(self.Alinear1(x))\n",
        "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
        "\n",
        "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
        "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
        "\n",
        "        q = Vx + (Ax - Ax.mean())\n",
        "\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WApt7r71sq8g"
      },
      "outputs": [],
      "source": [
        "#AGENT COMBINE CELL\n",
        "class DDDQNAgent:\n",
        "    def __init__(self, environment):\n",
        "        \"\"\"Set the hyperparameters for our agent\"\"\"\n",
        "\n",
        "        # Set the discount rate\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Set our initial exploration parameter epsilon\n",
        "        self.epsilon = 1\n",
        "\n",
        "        # Get number of actions available to the agent\n",
        "        self.n_actions = environment.action_space.n\n",
        "\n",
        "        # Initiate our replay memory\n",
        "        self.replay_buffer = deque(maxlen=50000)\n",
        "\n",
        "        # Create two model for DDQN algorithm\n",
        "        self.Net = DDQN(h=84, w=84, output_size=self.n_actions)\n",
        "        self.main_model = self.Net.to(GPU)\n",
        "        # self.target_model = DDQN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(GPU)\n",
        "        self.target_model=copy.deepcopy(self.main_model).to(GPU)\n",
        "        self.Net.apply(self.Net.init_weights)\n",
        "        self.target_model.load_state_dict(self.main_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # Set out optimizer\n",
        "        self.optimizer = optim.Adam(self.main_model.parameters(), lr=0.0003)\n",
        "\n",
        "    def convert_to_grey(self, image):\n",
        "        \"\"\"Convert the image to greyscale\"\"\"\n",
        "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    def crop_image(self, image, h_start=34, h_end=194, w_start=0, w_end=160):\n",
        "        \"\"\"Crop the image\"\"\"\n",
        "        return image[h_start:h_end, w_start:w_end]\n",
        "\n",
        "    def resize_reshape(self, image, height=84, width=84):\n",
        "        \"\"\"Resize and reshape our image\"\"\"\n",
        "        image = cv2.resize(image, (width, height))\n",
        "        return image.reshape(width, height)\n",
        "     \n",
        "\n",
        "    def preProcess(self, image):\n",
        "        img = self.convert_to_grey(image)\n",
        "        img = self.crop_image(img)\n",
        "        img = self.resize_reshape(img)\n",
        "\n",
        "        return img / 255\n",
        "\n",
        "    def random_action(self):\n",
        "      return random.randrange(self.n_actions)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            with torch.no_grad():\n",
        "                  state = torch.tensor(state, dtype=torch.float, device=GPU)\n",
        "                  state = state.unsqueeze(0)\n",
        "                  qs = self.main_model.forward(state)\n",
        "                  action = torch.argmax(qs).item()\n",
        "            return action\n",
        "        else:\n",
        "            action = random.randrange(self.n_actions)\n",
        "            return action\n",
        "\n",
        "    def predict(self, online, state):\n",
        "        if online:\n",
        "          return self.main_model(state)\n",
        "        else:\n",
        "          return self.target_model(state)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        #Delays learning until agent has sufficient experience.\n",
        "        if len(agent.replay_buffer) < 40000:\n",
        "            loss, max_q = [0, 0]\n",
        "            return loss, max_q\n",
        "\n",
        "        #Take minibatch of size 64 freom repay buffer\n",
        "        random_samples = random.sample(self.replay_buffer, 64) #Change 64 to change size of batch\n",
        "        state, action, reward, next_state, done = zip(*random_samples)\n",
        "\n",
        "\n",
        "        state = np.concatenate(state)\n",
        "        next_state = np.concatenate(next_state)\n",
        "\n",
        "        #Feed replays to Neural nets to observe expected q_values\n",
        "        state = torch.tensor(state, dtype=torch.float, device=GPU)\n",
        "        state_q_values = self.predict(True,state)\n",
        "        state_q_val_max = torch.max(state_q_values).item()\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float, device=GPU)\n",
        "        next_states_q_values = self.predict(True, next_state)\n",
        "        next_states_target_q_values = self.predict(False, next_state)\n",
        "\n",
        "\n",
        "        action = torch.tensor(action, dtype=torch.long, device=GPU)\n",
        "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(\n",
        "            1)).squeeze(1)\n",
        "        reward = torch.tensor(reward, dtype=torch.float, device=GPU)\n",
        "        done = torch.tensor(done, dtype=torch.float, device=GPU)\n",
        "\n",
        "        bellQ = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
        "\n",
        "        #Find difference between expected value and action value\n",
        "        difference_q_value = selected_q_value - bellQ.detach()\n",
        "        loss = (difference_q_value**2).mean()\n",
        "\n",
        "        #Use difference to update model weightings and hopefull improve performance\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        return loss, state_q_val_max\n",
        "\n",
        "    def storeResults(self, state, action, reward, nextState, done):\n",
        "        self.replay_buffer.append([state[None, :], action, reward, nextState[None, :], done])\n",
        "\n",
        "    def updateEpsilon(self):\n",
        "        if self.epsilon > 0.05:\n",
        "            self.epsilon *= 0.99\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve4vYDe3bozg",
        "outputId": "f0571080-3578-404e-ff3e-c7ee1240be3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Current Episode = 0,  Episode Reward = 0.0, Rolling Average = 0.0, Current Epsilon = 1, Total Step = 167\n",
            "3\n",
            "Current Episode = 1,  Episode Reward = 0.0, Rolling Average = 0.0, Current Epsilon = 1, Total Step = 331\n",
            "0\n",
            "Current Episode = 2,  Episode Reward = 1.0, Rolling Average = 0.3333333333333333, Current Epsilon = 1, Total Step = 578\n",
            "1\n",
            "Current Episode = 3,  Episode Reward = 0.0, Rolling Average = 0.25, Current Epsilon = 1, Total Step = 758\n",
            "1\n",
            "Current Episode = 4,  Episode Reward = 1.0, Rolling Average = 0.4, Current Epsilon = 1, Total Step = 980\n",
            "0\n",
            "Current Episode = 5,  Episode Reward = 3.0, Rolling Average = 0.8333333333333334, Current Epsilon = 0.99, Total Step = 1330\n",
            "2\n",
            "Current Episode = 6,  Episode Reward = 1.0, Rolling Average = 0.8571428571428571, Current Epsilon = 0.99, Total Step = 1572\n",
            "0\n",
            "Current Episode = 7,  Episode Reward = 1.0, Rolling Average = 0.875, Current Epsilon = 0.99, Total Step = 1790\n",
            "2\n",
            "Current Episode = 8,  Episode Reward = 3.0, Rolling Average = 1.1111111111111112, Current Epsilon = 0.9801, Total Step = 2104\n",
            "2\n",
            "Current Episode = 9,  Episode Reward = 1.0, Rolling Average = 1.1, Current Epsilon = 0.9801, Total Step = 2345\n",
            "3\n",
            "Current Episode = 10,  Episode Reward = 1.0, Rolling Average = 1.0909090909090908, Current Epsilon = 0.9801, Total Step = 2587\n",
            "0\n",
            "Current Episode = 11,  Episode Reward = 1.0, Rolling Average = 1.0833333333333333, Current Epsilon = 0.9801, Total Step = 2826\n",
            "0\n",
            "Current Episode = 12,  Episode Reward = 2.0, Rolling Average = 1.1538461538461537, Current Epsilon = 0.9702989999999999, Total Step = 3098\n",
            "1\n",
            "Current Episode = 13,  Episode Reward = 1.0, Rolling Average = 1.1428571428571428, Current Epsilon = 0.9702989999999999, Total Step = 3350\n",
            "1\n",
            "Current Episode = 14,  Episode Reward = 1.0, Rolling Average = 1.1333333333333333, Current Epsilon = 0.9702989999999999, Total Step = 3580\n",
            "2\n",
            "Current Episode = 15,  Episode Reward = 1.0, Rolling Average = 1.125, Current Epsilon = 0.9702989999999999, Total Step = 3829\n",
            "1\n",
            "Current Episode = 16,  Episode Reward = 3.0, Rolling Average = 1.2352941176470589, Current Epsilon = 0.96059601, Total Step = 4197\n",
            "2\n",
            "Current Episode = 17,  Episode Reward = 3.0, Rolling Average = 1.3333333333333333, Current Epsilon = 0.96059601, Total Step = 4542\n",
            "0\n",
            "Current Episode = 18,  Episode Reward = 0.0, Rolling Average = 1.263157894736842, Current Epsilon = 0.96059601, Total Step = 4720\n",
            "1\n",
            "Current Episode = 19,  Episode Reward = 1.0, Rolling Average = 1.25, Current Epsilon = 0.96059601, Total Step = 4963\n",
            "1\n",
            "Current Episode = 20,  Episode Reward = 7.0, Rolling Average = 1.5238095238095237, Current Epsilon = 0.9509900498999999, Total Step = 5445\n",
            "0\n",
            "Current Episode = 21,  Episode Reward = 3.0, Rolling Average = 1.5909090909090908, Current Epsilon = 0.9509900498999999, Total Step = 5805\n",
            "0\n",
            "Current Episode = 22,  Episode Reward = 0.0, Rolling Average = 1.5217391304347827, Current Epsilon = 0.9509900498999999, Total Step = 5979\n",
            "3\n",
            "Current Episode = 23,  Episode Reward = 0.0, Rolling Average = 1.4583333333333333, Current Epsilon = 0.9414801494009999, Total Step = 6150\n",
            "2\n",
            "Current Episode = 24,  Episode Reward = 1.0, Rolling Average = 1.44, Current Epsilon = 0.9414801494009999, Total Step = 6388\n",
            "0\n",
            "Current Episode = 25,  Episode Reward = 1.0, Rolling Average = 1.4230769230769231, Current Epsilon = 0.9414801494009999, Total Step = 6620\n",
            "0\n",
            "Current Episode = 26,  Episode Reward = 0.0, Rolling Average = 1.3703703703703705, Current Epsilon = 0.9414801494009999, Total Step = 6791\n",
            "3\n",
            "Current Episode = 27,  Episode Reward = 2.0, Rolling Average = 1.3928571428571428, Current Epsilon = 0.9320653479069899, Total Step = 7097\n",
            "2\n",
            "Current Episode = 28,  Episode Reward = 0.0, Rolling Average = 1.3448275862068966, Current Epsilon = 0.9320653479069899, Total Step = 7273\n",
            "2\n",
            "Current Episode = 29,  Episode Reward = 1.0, Rolling Average = 1.3333333333333333, Current Epsilon = 0.9320653479069899, Total Step = 7482\n",
            "1\n",
            "Current Episode = 30,  Episode Reward = 1.0, Rolling Average = 1.3225806451612903, Current Epsilon = 0.9320653479069899, Total Step = 7689\n",
            "0\n",
            "Current Episode = 31,  Episode Reward = 0.0, Rolling Average = 1.28125, Current Epsilon = 0.9320653479069899, Total Step = 7874\n",
            "0\n",
            "Current Episode = 32,  Episode Reward = 2.0, Rolling Average = 1.303030303030303, Current Epsilon = 0.92274469442792, Total Step = 8169\n",
            "2\n",
            "Current Episode = 33,  Episode Reward = 1.0, Rolling Average = 1.2941176470588236, Current Epsilon = 0.92274469442792, Total Step = 8404\n",
            "1\n",
            "Current Episode = 34,  Episode Reward = 0.0, Rolling Average = 1.2571428571428571, Current Epsilon = 0.92274469442792, Total Step = 8575\n",
            "3\n",
            "Current Episode = 35,  Episode Reward = 0.0, Rolling Average = 1.2222222222222223, Current Epsilon = 0.92274469442792, Total Step = 8759\n",
            "3\n",
            "Current Episode = 36,  Episode Reward = 0.0, Rolling Average = 1.1891891891891893, Current Epsilon = 0.92274469442792, Total Step = 8930\n",
            "3\n",
            "Current Episode = 37,  Episode Reward = 0.0, Rolling Average = 1.1578947368421053, Current Epsilon = 0.9135172474836407, Total Step = 9107\n",
            "0\n",
            "Current Episode = 38,  Episode Reward = 1.0, Rolling Average = 1.1538461538461537, Current Epsilon = 0.9135172474836407, Total Step = 9324\n",
            "3\n",
            "Current Episode = 39,  Episode Reward = 0.0, Rolling Average = 1.125, Current Epsilon = 0.9135172474836407, Total Step = 9513\n",
            "3\n",
            "Current Episode = 40,  Episode Reward = 2.0, Rolling Average = 1.146341463414634, Current Epsilon = 0.9135172474836407, Total Step = 9762\n",
            "1\n",
            "Current Episode = 41,  Episode Reward = 2.0, Rolling Average = 1.1666666666666667, Current Epsilon = 0.9043820750088043, Total Step = 10084\n",
            "0\n",
            "Current Episode = 42,  Episode Reward = 0.0, Rolling Average = 1.1395348837209303, Current Epsilon = 0.9043820750088043, Total Step = 10255\n",
            "3\n",
            "Current Episode = 43,  Episode Reward = 2.0, Rolling Average = 1.1590909090909092, Current Epsilon = 0.9043820750088043, Total Step = 10556\n",
            "0\n",
            "Current Episode = 44,  Episode Reward = 2.0, Rolling Average = 1.1777777777777778, Current Epsilon = 0.9043820750088043, Total Step = 10817\n",
            "0\n",
            "Current Episode = 45,  Episode Reward = 1.0, Rolling Average = 1.173913043478261, Current Epsilon = 0.8953382542587163, Total Step = 11042\n",
            "2\n",
            "Current Episode = 46,  Episode Reward = 1.0, Rolling Average = 1.1702127659574468, Current Epsilon = 0.8953382542587163, Total Step = 11282\n",
            "3\n",
            "Current Episode = 47,  Episode Reward = 0.0, Rolling Average = 1.1458333333333333, Current Epsilon = 0.8953382542587163, Total Step = 11461\n",
            "1\n",
            "Current Episode = 48,  Episode Reward = 1.0, Rolling Average = 1.1428571428571428, Current Epsilon = 0.8953382542587163, Total Step = 11710\n",
            "2\n",
            "Current Episode = 49,  Episode Reward = 0.0, Rolling Average = 1.12, Current Epsilon = 0.8953382542587163, Total Step = 11877\n",
            "2\n",
            "Current Episode = 50,  Episode Reward = 0.0, Rolling Average = 1.0980392156862746, Current Epsilon = 0.8863848717161291, Total Step = 12077\n",
            "0\n",
            "Current Episode = 51,  Episode Reward = 1.0, Rolling Average = 1.0961538461538463, Current Epsilon = 0.8863848717161291, Total Step = 12288\n",
            "0\n",
            "Current Episode = 52,  Episode Reward = 0.0, Rolling Average = 1.0754716981132075, Current Epsilon = 0.8863848717161291, Total Step = 12472\n",
            "0\n",
            "Current Episode = 53,  Episode Reward = 0.0, Rolling Average = 1.0555555555555556, Current Epsilon = 0.8863848717161291, Total Step = 12671\n",
            "0\n",
            "Current Episode = 54,  Episode Reward = 1.0, Rolling Average = 1.0545454545454545, Current Epsilon = 0.8863848717161291, Total Step = 12882\n",
            "1\n",
            "Current Episode = 55,  Episode Reward = 3.0, Rolling Average = 1.0892857142857142, Current Epsilon = 0.8775210229989678, Total Step = 13171\n",
            "2\n",
            "Current Episode = 56,  Episode Reward = 2.0, Rolling Average = 1.105263157894737, Current Epsilon = 0.8775210229989678, Total Step = 13458\n",
            "0\n",
            "Current Episode = 57,  Episode Reward = 0.0, Rolling Average = 1.0862068965517242, Current Epsilon = 0.8775210229989678, Total Step = 13620\n",
            "1\n",
            "Current Episode = 58,  Episode Reward = 1.0, Rolling Average = 1.0847457627118644, Current Epsilon = 0.8775210229989678, Total Step = 13841\n",
            "2\n",
            "Current Episode = 59,  Episode Reward = 2.0, Rolling Average = 1.1, Current Epsilon = 0.8687458127689781, Total Step = 14115\n",
            "2\n",
            "Current Episode = 60,  Episode Reward = 0.0, Rolling Average = 1.0819672131147542, Current Epsilon = 0.8687458127689781, Total Step = 14289\n",
            "2\n",
            "Current Episode = 61,  Episode Reward = 0.0, Rolling Average = 1.064516129032258, Current Epsilon = 0.8687458127689781, Total Step = 14452\n",
            "3\n",
            "Current Episode = 62,  Episode Reward = 1.0, Rolling Average = 1.0634920634920635, Current Epsilon = 0.8687458127689781, Total Step = 14677\n",
            "0\n",
            "Current Episode = 63,  Episode Reward = 2.0, Rolling Average = 1.078125, Current Epsilon = 0.8687458127689781, Total Step = 14936\n",
            "2\n",
            "Current Episode = 64,  Episode Reward = 1.0, Rolling Average = 1.0769230769230769, Current Epsilon = 0.8600583546412883, Total Step = 15174\n",
            "3\n",
            "Current Episode = 65,  Episode Reward = 1.0, Rolling Average = 1.0757575757575757, Current Epsilon = 0.8600583546412883, Total Step = 15389\n",
            "1\n",
            "Current Episode = 66,  Episode Reward = 0.0, Rolling Average = 1.0597014925373134, Current Epsilon = 0.8600583546412883, Total Step = 15592\n",
            "2\n",
            "Current Episode = 67,  Episode Reward = 0.0, Rolling Average = 1.0441176470588236, Current Epsilon = 0.8600583546412883, Total Step = 15772\n",
            "2\n",
            "Current Episode = 68,  Episode Reward = 2.0, Rolling Average = 1.0579710144927537, Current Epsilon = 0.8514577710948754, Total Step = 16060\n",
            "3\n",
            "Current Episode = 69,  Episode Reward = 1.0, Rolling Average = 1.0571428571428572, Current Epsilon = 0.8514577710948754, Total Step = 16330\n",
            "1\n",
            "Current Episode = 70,  Episode Reward = 1.0, Rolling Average = 1.056338028169014, Current Epsilon = 0.8514577710948754, Total Step = 16562\n",
            "1\n",
            "Current Episode = 71,  Episode Reward = 3.0, Rolling Average = 1.0833333333333333, Current Epsilon = 0.8514577710948754, Total Step = 16912\n",
            "1\n",
            "Current Episode = 72,  Episode Reward = 1.0, Rolling Average = 1.082191780821918, Current Epsilon = 0.8429431933839266, Total Step = 17134\n",
            "2\n",
            "Current Episode = 73,  Episode Reward = 0.0, Rolling Average = 1.0675675675675675, Current Epsilon = 0.8429431933839266, Total Step = 17302\n",
            "2\n",
            "Current Episode = 74,  Episode Reward = 1.0, Rolling Average = 1.0666666666666667, Current Epsilon = 0.8429431933839266, Total Step = 17542\n",
            "1\n",
            "Current Episode = 75,  Episode Reward = 1.0, Rolling Average = 1.0657894736842106, Current Epsilon = 0.8429431933839266, Total Step = 17760\n",
            "1\n",
            "Current Episode = 76,  Episode Reward = 1.0, Rolling Average = 1.0649350649350648, Current Epsilon = 0.8429431933839266, Total Step = 17977\n",
            "3\n",
            "Current Episode = 77,  Episode Reward = 1.0, Rolling Average = 1.064102564102564, Current Epsilon = 0.8345137614500874, Total Step = 18195\n",
            "2\n",
            "Current Episode = 78,  Episode Reward = 0.0, Rolling Average = 1.0506329113924051, Current Epsilon = 0.8345137614500874, Total Step = 18383\n",
            "3\n",
            "Current Episode = 79,  Episode Reward = 3.0, Rolling Average = 1.075, Current Epsilon = 0.8345137614500874, Total Step = 18720\n",
            "0\n",
            "Current Episode = 80,  Episode Reward = 1.0, Rolling Average = 1.0740740740740742, Current Epsilon = 0.8345137614500874, Total Step = 18954\n",
            "1\n",
            "Current Episode = 81,  Episode Reward = 1.0, Rolling Average = 1.0731707317073171, Current Epsilon = 0.8261686238355865, Total Step = 19202\n",
            "2\n",
            "Current Episode = 82,  Episode Reward = 1.0, Rolling Average = 1.072289156626506, Current Epsilon = 0.8261686238355865, Total Step = 19423\n",
            "0\n",
            "Current Episode = 83,  Episode Reward = 2.0, Rolling Average = 1.0833333333333333, Current Epsilon = 0.8261686238355865, Total Step = 19676\n",
            "3\n",
            "Current Episode = 84,  Episode Reward = 4.0, Rolling Average = 1.1176470588235294, Current Epsilon = 0.8179069375972307, Total Step = 20051\n",
            "1\n",
            "Current Episode = 85,  Episode Reward = 0.0, Rolling Average = 1.1046511627906976, Current Epsilon = 0.8179069375972307, Total Step = 20227\n",
            "3\n",
            "Current Episode = 86,  Episode Reward = 2.0, Rolling Average = 1.1149425287356323, Current Epsilon = 0.8179069375972307, Total Step = 20504\n",
            "0\n",
            "Current Episode = 87,  Episode Reward = 2.0, Rolling Average = 1.125, Current Epsilon = 0.8179069375972307, Total Step = 20782\n",
            "0\n",
            "Current Episode = 88,  Episode Reward = 5.0, Rolling Average = 1.1685393258426966, Current Epsilon = 0.8097278682212583, Total Step = 21247\n",
            "0\n",
            "Current Episode = 89,  Episode Reward = 1.0, Rolling Average = 1.1666666666666667, Current Epsilon = 0.8097278682212583, Total Step = 21484\n",
            "3\n",
            "Current Episode = 90,  Episode Reward = 0.0, Rolling Average = 1.1538461538461537, Current Epsilon = 0.8097278682212583, Total Step = 21663\n",
            "3\n",
            "Current Episode = 91,  Episode Reward = 3.0, Rolling Average = 1.173913043478261, Current Epsilon = 0.8097278682212583, Total Step = 21995\n",
            "0\n",
            "Current Episode = 92,  Episode Reward = 3.0, Rolling Average = 1.1935483870967742, Current Epsilon = 0.8016305895390458, Total Step = 22305\n",
            "1\n",
            "Current Episode = 93,  Episode Reward = 0.0, Rolling Average = 1.1808510638297873, Current Epsilon = 0.8016305895390458, Total Step = 22501\n",
            "2\n",
            "Current Episode = 94,  Episode Reward = 0.0, Rolling Average = 1.168421052631579, Current Epsilon = 0.8016305895390458, Total Step = 22674\n",
            "3\n",
            "Current Episode = 95,  Episode Reward = 4.0, Rolling Average = 1.1979166666666667, Current Epsilon = 0.7936142836436553, Total Step = 23107\n",
            "1\n",
            "Current Episode = 96,  Episode Reward = 2.0, Rolling Average = 1.2061855670103092, Current Epsilon = 0.7936142836436553, Total Step = 23377\n",
            "2\n",
            "Current Episode = 97,  Episode Reward = 0.0, Rolling Average = 1.1938775510204083, Current Epsilon = 0.7936142836436553, Total Step = 23558\n",
            "1\n",
            "Current Episode = 98,  Episode Reward = 2.0, Rolling Average = 1.202020202020202, Current Epsilon = 0.7936142836436553, Total Step = 23827\n",
            "2\n",
            "Current Episode = 99,  Episode Reward = 4.0, Rolling Average = 1.23, Current Epsilon = 0.7856781408072188, Total Step = 24206\n",
            "2\n",
            "Current Episode = 100,  Episode Reward = 0.0, Rolling Average = 1.23, Current Epsilon = 0.7856781408072188, Total Step = 24389\n",
            "2\n",
            "Current Episode = 101,  Episode Reward = 2.0, Rolling Average = 1.25, Current Epsilon = 0.7856781408072188, Total Step = 24636\n",
            "3\n",
            "Current Episode = 102,  Episode Reward = 0.0, Rolling Average = 1.24, Current Epsilon = 0.7856781408072188, Total Step = 24829\n",
            "2\n",
            "Current Episode = 103,  Episode Reward = 3.0, Rolling Average = 1.27, Current Epsilon = 0.7778213593991465, Total Step = 25127\n",
            "2\n",
            "Current Episode = 104,  Episode Reward = 4.0, Rolling Average = 1.3, Current Epsilon = 0.7778213593991465, Total Step = 25510\n",
            "1\n",
            "Current Episode = 105,  Episode Reward = 2.0, Rolling Average = 1.29, Current Epsilon = 0.7778213593991465, Total Step = 25764\n",
            "1\n",
            "Current Episode = 106,  Episode Reward = 2.0, Rolling Average = 1.3, Current Epsilon = 0.7700431458051551, Total Step = 26058\n",
            "0\n",
            "Current Episode = 107,  Episode Reward = 1.0, Rolling Average = 1.3, Current Epsilon = 0.7700431458051551, Total Step = 26269\n",
            "3\n",
            "Current Episode = 108,  Episode Reward = 1.0, Rolling Average = 1.28, Current Epsilon = 0.7700431458051551, Total Step = 26487\n",
            "1\n",
            "Current Episode = 109,  Episode Reward = 0.0, Rolling Average = 1.27, Current Epsilon = 0.7700431458051551, Total Step = 26701\n",
            "3\n",
            "Current Episode = 110,  Episode Reward = 1.0, Rolling Average = 1.27, Current Epsilon = 0.7700431458051551, Total Step = 26920\n",
            "3\n",
            "Current Episode = 111,  Episode Reward = 10.0, Rolling Average = 1.36, Current Epsilon = 0.7623427143471035, Total Step = 27493\n",
            "2\n",
            "Current Episode = 112,  Episode Reward = 1.0, Rolling Average = 1.35, Current Epsilon = 0.7623427143471035, Total Step = 27757\n",
            "1\n",
            "Current Episode = 113,  Episode Reward = 4.0, Rolling Average = 1.38, Current Epsilon = 0.7547192872036325, Total Step = 28127\n",
            "1\n",
            "Current Episode = 114,  Episode Reward = 1.0, Rolling Average = 1.38, Current Epsilon = 0.7547192872036325, Total Step = 28357\n",
            "2\n",
            "Current Episode = 115,  Episode Reward = 0.0, Rolling Average = 1.37, Current Epsilon = 0.7547192872036325, Total Step = 28544\n",
            "3\n",
            "Current Episode = 116,  Episode Reward = 0.0, Rolling Average = 1.34, Current Epsilon = 0.7547192872036325, Total Step = 28719\n",
            "0\n",
            "Current Episode = 117,  Episode Reward = 0.0, Rolling Average = 1.31, Current Epsilon = 0.7547192872036325, Total Step = 28895\n",
            "0\n",
            "Current Episode = 118,  Episode Reward = 1.0, Rolling Average = 1.32, Current Epsilon = 0.7471720943315961, Total Step = 29132\n",
            "1\n",
            "Current Episode = 119,  Episode Reward = 2.0, Rolling Average = 1.33, Current Epsilon = 0.7471720943315961, Total Step = 29418\n",
            "1\n",
            "Current Episode = 120,  Episode Reward = 1.0, Rolling Average = 1.27, Current Epsilon = 0.7471720943315961, Total Step = 29647\n",
            "1\n",
            "Current Episode = 121,  Episode Reward = 2.0, Rolling Average = 1.26, Current Epsilon = 0.7471720943315961, Total Step = 29909\n",
            "3\n",
            "Current Episode = 122,  Episode Reward = 4.0, Rolling Average = 1.3, Current Epsilon = 0.7397003733882802, Total Step = 30303\n",
            "3\n",
            "Current Episode = 123,  Episode Reward = 1.0, Rolling Average = 1.31, Current Epsilon = 0.7397003733882802, Total Step = 30521\n",
            "1\n",
            "Current Episode = 124,  Episode Reward = 1.0, Rolling Average = 1.31, Current Epsilon = 0.7397003733882802, Total Step = 30745\n",
            "2\n",
            "Current Episode = 125,  Episode Reward = 2.0, Rolling Average = 1.32, Current Epsilon = 0.7323033696543974, Total Step = 31028\n",
            "0\n",
            "Current Episode = 126,  Episode Reward = 2.0, Rolling Average = 1.34, Current Epsilon = 0.7323033696543974, Total Step = 31321\n",
            "2\n",
            "Current Episode = 127,  Episode Reward = 1.0, Rolling Average = 1.33, Current Epsilon = 0.7323033696543974, Total Step = 31558\n",
            "1\n",
            "Current Episode = 128,  Episode Reward = 3.0, Rolling Average = 1.36, Current Epsilon = 0.7323033696543974, Total Step = 31903\n",
            "0\n",
            "Current Episode = 129,  Episode Reward = 0.0, Rolling Average = 1.35, Current Epsilon = 0.7249803359578534, Total Step = 32078\n",
            "2\n",
            "Current Episode = 130,  Episode Reward = 1.0, Rolling Average = 1.35, Current Epsilon = 0.7249803359578534, Total Step = 32290\n",
            "3\n",
            "Current Episode = 131,  Episode Reward = 3.0, Rolling Average = 1.38, Current Epsilon = 0.7249803359578534, Total Step = 32626\n",
            "1\n",
            "Current Episode = 132,  Episode Reward = 1.0, Rolling Average = 1.37, Current Epsilon = 0.7249803359578534, Total Step = 32840\n",
            "1\n",
            "Current Episode = 133,  Episode Reward = 0.0, Rolling Average = 1.36, Current Epsilon = 0.7177305325982748, Total Step = 33025\n",
            "0\n",
            "Current Episode = 134,  Episode Reward = 0.0, Rolling Average = 1.36, Current Epsilon = 0.7177305325982748, Total Step = 33225\n",
            "1\n",
            "Current Episode = 135,  Episode Reward = 2.0, Rolling Average = 1.38, Current Epsilon = 0.7177305325982748, Total Step = 33477\n",
            "0\n",
            "Current Episode = 136,  Episode Reward = 1.0, Rolling Average = 1.39, Current Epsilon = 0.7177305325982748, Total Step = 33712\n",
            "1\n",
            "Current Episode = 137,  Episode Reward = 0.0, Rolling Average = 1.39, Current Epsilon = 0.7177305325982748, Total Step = 33879\n",
            "2\n",
            "Current Episode = 138,  Episode Reward = 2.0, Rolling Average = 1.4, Current Epsilon = 0.7105532272722921, Total Step = 34148\n",
            "2\n",
            "Current Episode = 139,  Episode Reward = 0.0, Rolling Average = 1.4, Current Epsilon = 0.7105532272722921, Total Step = 34321\n",
            "1\n",
            "Current Episode = 140,  Episode Reward = 0.0, Rolling Average = 1.38, Current Epsilon = 0.7105532272722921, Total Step = 34509\n",
            "3\n",
            "Current Episode = 141,  Episode Reward = 0.0, Rolling Average = 1.36, Current Epsilon = 0.7105532272722921, Total Step = 34698\n",
            "1\n",
            "Current Episode = 142,  Episode Reward = 1.0, Rolling Average = 1.37, Current Epsilon = 0.7105532272722921, Total Step = 34916\n",
            "2\n",
            "Current Episode = 143,  Episode Reward = 0.0, Rolling Average = 1.35, Current Epsilon = 0.7034476949995692, Total Step = 35111\n",
            "0\n",
            "Current Episode = 144,  Episode Reward = 1.0, Rolling Average = 1.34, Current Epsilon = 0.7034476949995692, Total Step = 35330\n",
            "0\n",
            "Current Episode = 145,  Episode Reward = 2.0, Rolling Average = 1.35, Current Epsilon = 0.7034476949995692, Total Step = 35578\n",
            "2\n",
            "Current Episode = 146,  Episode Reward = 0.0, Rolling Average = 1.34, Current Epsilon = 0.7034476949995692, Total Step = 35778\n",
            "0\n",
            "Current Episode = 147,  Episode Reward = 1.0, Rolling Average = 1.35, Current Epsilon = 0.6964132180495735, Total Step = 36009\n",
            "1\n",
            "Current Episode = 148,  Episode Reward = 1.0, Rolling Average = 1.35, Current Epsilon = 0.6964132180495735, Total Step = 36221\n",
            "0\n",
            "Current Episode = 149,  Episode Reward = 1.0, Rolling Average = 1.36, Current Epsilon = 0.6964132180495735, Total Step = 36464\n",
            "0\n",
            "Current Episode = 150,  Episode Reward = 2.0, Rolling Average = 1.38, Current Epsilon = 0.6964132180495735, Total Step = 36732\n",
            "1\n",
            "Current Episode = 151,  Episode Reward = 0.0, Rolling Average = 1.37, Current Epsilon = 0.6964132180495735, Total Step = 36918\n",
            "0\n",
            "Current Episode = 152,  Episode Reward = 0.0, Rolling Average = 1.37, Current Epsilon = 0.6894490858690777, Total Step = 37092\n",
            "2\n",
            "Current Episode = 153,  Episode Reward = 2.0, Rolling Average = 1.39, Current Epsilon = 0.6894490858690777, Total Step = 37346\n",
            "2\n",
            "Current Episode = 154,  Episode Reward = 2.0, Rolling Average = 1.4, Current Epsilon = 0.6894490858690777, Total Step = 37632\n",
            "0\n",
            "Current Episode = 155,  Episode Reward = 1.0, Rolling Average = 1.38, Current Epsilon = 0.6894490858690777, Total Step = 37865\n",
            "3\n",
            "Current Episode = 156,  Episode Reward = 0.0, Rolling Average = 1.36, Current Epsilon = 0.682554595010387, Total Step = 38055\n",
            "0\n",
            "Current Episode = 157,  Episode Reward = 2.0, Rolling Average = 1.38, Current Epsilon = 0.682554595010387, Total Step = 38351\n",
            "2\n",
            "Current Episode = 158,  Episode Reward = 0.0, Rolling Average = 1.37, Current Epsilon = 0.682554595010387, Total Step = 38536\n",
            "0\n",
            "Current Episode = 159,  Episode Reward = 0.0, Rolling Average = 1.35, Current Epsilon = 0.682554595010387, Total Step = 38735\n",
            "3\n",
            "Current Episode = 160,  Episode Reward = 5.0, Rolling Average = 1.4, Current Epsilon = 0.6757290490602831, Total Step = 39170\n",
            "1\n",
            "Current Episode = 161,  Episode Reward = 0.0, Rolling Average = 1.4, Current Epsilon = 0.6757290490602831, Total Step = 39371\n",
            "1\n",
            "Current Episode = 162,  Episode Reward = 0.0, Rolling Average = 1.39, Current Epsilon = 0.6757290490602831, Total Step = 39560\n",
            "2\n",
            "Current Episode = 163,  Episode Reward = 1.0, Rolling Average = 1.38, Current Epsilon = 0.6757290490602831, Total Step = 39777\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Breakout-v4', render_mode = None)\n",
        "agent = DDDQNAgent(env)\n",
        "rolling_100 = deque(maxlen=100)\n",
        "total_iter = 1\n",
        "\n",
        "for episode in range(100000): #Loops for passed range number of total episodes.\n",
        "\n",
        "    #Grab the first frame, and then jam 4 of them together rather unceremoniously. Standard practice for Atari Games. Helps to demonstrate velocity.\n",
        "    state = env.reset()\n",
        "    print(agent.random_action())\n",
        "    next_state, reward, dead, _, _ = env.step(2)\n",
        "    state = agent.preProcess(next_state)\n",
        "    state = np.stack((state, state, state, state))\n",
        "\n",
        "    #Declaring in per-episode totals\n",
        "    Max_Qs = 0 \n",
        "    episode_reward = 0\n",
        "    episode_loss = 0\n",
        "\n",
        "    for iter in range(100000): #Very ambitious limit to number of steps in a given episode. Will be very surprised if we get anywhere near this.\n",
        "\n",
        "        #Classic Sarsa style step, take action and observe output from environment. Then store those results for experience recall\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, dead, _, _= env.step(action)\n",
        "        next_state = agent.preProcess(next_state)\n",
        "        next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
        "        agent.storeResults(state, action, reward, next_state, dead)\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        #Now we update the weights of the agent.\n",
        "        loss, max_q = agent.train()\n",
        "        episode_loss += loss\n",
        "        Max_Qs += max_q\n",
        "\n",
        "        total_iter += 1\n",
        "\n",
        "        #Update the epsilon as we go to improve late training performance. Exploration/Exploitation trade off.\n",
        "        if total_iter % 1000 == 0:\n",
        "            agent.updateEpsilon()\n",
        "\n",
        "        if dead:\n",
        "            #Episode has ended. DDDQNAgent has run out of lives.\n",
        "\n",
        "            #Get episode statistics ready.\n",
        "            rolling_100.append(episode_reward)\n",
        "            avg_max_q_val = Max_Qs / iter\n",
        "\n",
        "            #Log for monitoring\n",
        "            print(f\"Current Episode = {episode},  Episode Reward = {episode_reward}, Rolling Average = {np.mean(rolling_100)}, Current Epsilon = {agent.epsilon}, Total Step = {total_iter}\")\n",
        "\n",
        "            #Send to WandB for remote monitoring and summary graphing.\n",
        "            \"\"\"\n",
        "            wandb.log({'Episode': episode, \\\n",
        "                           \"Average reward\": np.mean(rolling_100), \\\n",
        "                           \"Total Loss\": episode_loss, \\\n",
        "                           \"Average maxQ\": round(avg_max_q_val, 2), \\\n",
        "                           \"total_step\": total_iter})\n",
        "            \"\"\"\n",
        "            #Save model as pkl to store progress / use later to play.\n",
        "            if episode % 20 == 0:\n",
        "                file_name = \"./DQNBreakOut-\" + str(episode) + \"-\" + str(round(np.mean(rolling_100))) + '.pkl'\n",
        "                torch.save(agent.main_model.state_dict(), file_name)\n",
        "            \n",
        "            #Make sure target and online model weights match\n",
        "            agent.target_model.load_state_dict(agent.main_model.state_dict())\n",
        "\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-AQNniGkltD"
      },
      "source": [
        "Acknowledgements:\n",
        "\n",
        "  Playing Atari with Deep Learning - Deepmind - Available at:  https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
        "\n",
        "  Official Pytorch DQN tutorial - Available at: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "  \n",
        "  Neural Network and Hyperparameters informed by:\n",
        "  https://lzzmm.github.io/2021/11/05/breakout/\n",
        "\n",
        "  Adaptation of model to breakout learned from:\n",
        "\n",
        "  https://github.com/AdrianHsu/breakout-Deep-Q-Network\n",
        "\n",
        "  https://keras.io/examples/rl/deep_q_network_breakout/\n",
        "\n",
        "  https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "\n",
        "  Use of pytorch in this environment informed by:\n",
        "\n",
        "  https://www.mlq.ai/deep-reinforcement-learning-pytorch-implementation/\n",
        "\n",
        "  https://github.com/iKintosh/DQN-breakout-Pytorch\n",
        "\n",
        "  https://github.com/bhctsntrk/OpenAIPong-DQN\n",
        "\n",
        "  https://github.com/jasonbian97/Deep-Q-Learning-Atari-Pytorch\n",
        "\n",
        "\n",
        "  Prioritised replay researched from (Not able to implement):\n",
        "\n",
        "  https://github.com/sfyzsr/Reinforcement-Learning-for-Atari\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}